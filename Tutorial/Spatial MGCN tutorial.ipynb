{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from utils import *\n",
    "from models import Spatial_MGCN\n",
    "import os\n",
    "import argparse\n",
    "from config import Config\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time,psutil,tracemalloc\n",
    "from scipy.sparse import issparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def mk_dir(input_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        os.makedirs(input_path)\n",
    "    return input_path\n",
    "\n",
    "\n",
    "def load_DLPFC_data(dataset):\n",
    "    print(\"load DLPFC dataset:\")\n",
    "    path = \"../../Dataset/Spatial_MGCN_generate/DLPFC/\" + dataset + \"/Spatial_MGCN.h5ad\"\n",
    "    adata = sc.read_h5ad(path)\n",
    "\n",
    "    adata.X=adata.X.todense() if issparse(adata.X) else adata.X\n",
    "    features = torch.FloatTensor(adata.X)\n",
    "    labels = adata.obs['ground_truth']\n",
    "    fadj = adata.obsm['fadj']\n",
    "    sadj = adata.obsm['sadj']\n",
    "    nfadj = normalize_sparse_matrix(fadj + sp.eye(fadj.shape[0]))\n",
    "    nfadj = sparse_mx_to_torch_sparse_tensor(nfadj)\n",
    "    nsadj = normalize_sparse_matrix(sadj + sp.eye(sadj.shape[0]))\n",
    "    nsadj = sparse_mx_to_torch_sparse_tensor(nsadj)\n",
    "    graph_nei = torch.LongTensor(adata.obsm['graph_nei'])\n",
    "    graph_neg = torch.LongTensor(adata.obsm['graph_neg'])\n",
    "    print(\"done\")\n",
    "    return adata, features, labels, nfadj, nsadj, graph_nei, graph_neg"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def load_data(dataset):\n",
    "    if dataset.startswith('15'):\n",
    "        path = \"../../Dataset/Spatial_MGCN_generate/DLPFC/\" + dataset + \"/\"\n",
    "    else:\n",
    "        path = \"../../Dataset/Spatial_MGCN_generate/\" + dataset + \"/\"\n",
    "\n",
    "\n",
    "    adata = sc.read_h5ad(f'{path}Spatial_MGCN.h5ad')\n",
    "    features = torch.FloatTensor(adata.X)\n",
    "    if dataset=='Stereo':\n",
    "        adata.obs['ground_truth']=adata.obs['Annotation']\n",
    "    labels = adata.obs['ground_truth']\n",
    "    fadj = adata.obsm['fadj']\n",
    "    sadj = adata.obsm['sadj']\n",
    "    nfadj = normalize_sparse_matrix(fadj + sp.eye(fadj.shape[0]))\n",
    "    nfadj = sparse_mx_to_torch_sparse_tensor(nfadj)\n",
    "    nsadj = normalize_sparse_matrix(sadj + sp.eye(sadj.shape[0]))\n",
    "    nsadj = sparse_mx_to_torch_sparse_tensor(nsadj)\n",
    "    graph_nei = torch.LongTensor(adata.obsm['graph_nei'])\n",
    "    graph_neg = torch.LongTensor(adata.obsm['graph_neg'])\n",
    "    print(\"done\")\n",
    "    return adata, features, labels, nsadj, nfadj, graph_nei, graph_neg\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    com1, com2, emb, pi, disp, mean = model(features, sadj, fadj)\n",
    "    zinb_loss = ZINB(pi, theta=disp, ridge_lambda=0).loss(features, mean, mean=True)\n",
    "    reg_loss = regularization_loss(emb, graph_nei, graph_neg)\n",
    "    con_loss = consistency_loss(com1, com2)\n",
    "    total_loss = config.alpha * zinb_loss + config.beta * con_loss + config.gamma * reg_loss\n",
    "    emb = pd.DataFrame(emb.cpu().detach().numpy()).fillna(0).values\n",
    "    mean = pd.DataFrame(mean.cpu().detach().numpy()).fillna(0).values\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    return emb, mean, zinb_loss, reg_loss, con_loss, total_loss\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load DLPFC dataset:\n",
      "done\n",
      "config pathï¼š J:\\Benchmark_ST_analysis\\Benchmark_SRT-main\\Spatial_MGCN\\./config/DLPFC.ini\n",
      "===Train epoch1====\n",
      "151673   0.001   1.0   10.0   0.1\n",
      "151673  epoch:  0  zinb_loss = 0.79  reg_loss = 0.60  con_loss = 0.38  total_loss = 4.64\n",
      "151673  epoch:  1  zinb_loss = 0.78  reg_loss = 0.55  con_loss = 0.29  total_loss = 3.74\n",
      "151673  epoch:  2  zinb_loss = 0.78  reg_loss = 0.52  con_loss = 0.19  total_loss = 2.69\n",
      "151673  epoch:  3  zinb_loss = 0.77  reg_loss = 0.49  con_loss = 0.16  total_loss = 2.41\n",
      "151673  epoch:  4  zinb_loss = 0.77  reg_loss = 0.46  con_loss = 0.14  total_loss = 2.17\n",
      "151673  epoch:  5  zinb_loss = 0.76  reg_loss = 0.46  con_loss = 0.13  total_loss = 2.11\n",
      "151673  epoch:  6  zinb_loss = 0.76  reg_loss = 0.44  con_loss = 0.12  total_loss = 2.01\n",
      "151673  epoch:  7  zinb_loss = 0.75  reg_loss = 0.41  con_loss = 0.11  total_loss = 1.90\n",
      "151673  epoch:  8  zinb_loss = 0.75  reg_loss = 0.41  con_loss = 0.10  total_loss = 1.84\n",
      "151673  epoch:  9  zinb_loss = 0.75  reg_loss = 0.40  con_loss = 0.10  total_loss = 1.75\n",
      "151673  epoch:  10  zinb_loss = 0.74  reg_loss = 0.39  con_loss = 0.09  total_loss = 1.65\n",
      "151673  epoch:  11  zinb_loss = 0.74  reg_loss = 0.39  con_loss = 0.08  total_loss = 1.61\n",
      "151673  epoch:  12  zinb_loss = 0.74  reg_loss = 0.38  con_loss = 0.08  total_loss = 1.58\n",
      "151673  epoch:  13  zinb_loss = 0.73  reg_loss = 0.38  con_loss = 0.08  total_loss = 1.53\n",
      "151673  epoch:  14  zinb_loss = 0.73  reg_loss = 0.38  con_loss = 0.07  total_loss = 1.51\n",
      "151673  epoch:  15  zinb_loss = 0.73  reg_loss = 0.38  con_loss = 0.07  total_loss = 1.50\n",
      "151673  epoch:  16  zinb_loss = 0.73  reg_loss = 0.37  con_loss = 0.07  total_loss = 1.46\n",
      "151673  epoch:  17  zinb_loss = 0.72  reg_loss = 0.37  con_loss = 0.07  total_loss = 1.43\n",
      "151673  epoch:  18  zinb_loss = 0.72  reg_loss = 0.37  con_loss = 0.07  total_loss = 1.42\n",
      "151673  epoch:  19  zinb_loss = 0.72  reg_loss = 0.37  con_loss = 0.06  total_loss = 1.39\n",
      "151673  epoch:  20  zinb_loss = 0.72  reg_loss = 0.37  con_loss = 0.06  total_loss = 1.37\n",
      "151673  epoch:  21  zinb_loss = 0.72  reg_loss = 0.37  con_loss = 0.06  total_loss = 1.35\n",
      "151673  epoch:  22  zinb_loss = 0.71  reg_loss = 0.37  con_loss = 0.06  total_loss = 1.34\n",
      "151673  epoch:  23  zinb_loss = 0.71  reg_loss = 0.37  con_loss = 0.06  total_loss = 1.32\n",
      "151673  epoch:  24  zinb_loss = 0.71  reg_loss = 0.37  con_loss = 0.06  total_loss = 1.31\n",
      "151673  epoch:  25  zinb_loss = 0.71  reg_loss = 0.37  con_loss = 0.06  total_loss = 1.30\n",
      "151673  epoch:  26  zinb_loss = 0.71  reg_loss = 0.37  con_loss = 0.05  total_loss = 1.28\n",
      "151673  epoch:  27  zinb_loss = 0.70  reg_loss = 0.37  con_loss = 0.05  total_loss = 1.27\n",
      "151673  epoch:  28  zinb_loss = 0.70  reg_loss = 0.37  con_loss = 0.05  total_loss = 1.26\n",
      "151673  epoch:  29  zinb_loss = 0.70  reg_loss = 0.37  con_loss = 0.05  total_loss = 1.25\n",
      "151673  epoch:  30  zinb_loss = 0.70  reg_loss = 0.37  con_loss = 0.05  total_loss = 1.25\n",
      "151673  epoch:  31  zinb_loss = 0.70  reg_loss = 0.37  con_loss = 0.05  total_loss = 1.24\n",
      "151673  epoch:  32  zinb_loss = 0.70  reg_loss = 0.37  con_loss = 0.05  total_loss = 1.23\n",
      "151673  epoch:  33  zinb_loss = 0.70  reg_loss = 0.37  con_loss = 0.05  total_loss = 1.22\n",
      "151673  epoch:  34  zinb_loss = 0.69  reg_loss = 0.37  con_loss = 0.05  total_loss = 1.22\n",
      "151673  epoch:  35  zinb_loss = 0.69  reg_loss = 0.37  con_loss = 0.05  total_loss = 1.21\n",
      "151673  epoch:  36  zinb_loss = 0.69  reg_loss = 0.37  con_loss = 0.05  total_loss = 1.20\n",
      "151673  epoch:  37  zinb_loss = 0.69  reg_loss = 0.37  con_loss = 0.05  total_loss = 1.20\n",
      "151673  epoch:  38  zinb_loss = 0.69  reg_loss = 0.37  con_loss = 0.05  total_loss = 1.19\n",
      "151673  epoch:  39  zinb_loss = 0.69  reg_loss = 0.37  con_loss = 0.05  total_loss = 1.18\n",
      "151673  epoch:  40  zinb_loss = 0.69  reg_loss = 0.37  con_loss = 0.05  total_loss = 1.18\n",
      "151673  epoch:  41  zinb_loss = 0.69  reg_loss = 0.37  con_loss = 0.04  total_loss = 1.17\n",
      "151673  epoch:  42  zinb_loss = 0.69  reg_loss = 0.37  con_loss = 0.04  total_loss = 1.16\n",
      "151673  epoch:  43  zinb_loss = 0.69  reg_loss = 0.37  con_loss = 0.04  total_loss = 1.16\n",
      "151673  epoch:  44  zinb_loss = 0.68  reg_loss = 0.37  con_loss = 0.04  total_loss = 1.16\n",
      "151673  epoch:  45  zinb_loss = 0.68  reg_loss = 0.37  con_loss = 0.04  total_loss = 1.15\n",
      "151673  epoch:  46  zinb_loss = 0.68  reg_loss = 0.37  con_loss = 0.04  total_loss = 1.15\n",
      "151673  epoch:  47  zinb_loss = 0.68  reg_loss = 0.37  con_loss = 0.04  total_loss = 1.14\n",
      "151673  epoch:  48  zinb_loss = 0.68  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.14\n",
      "151673  epoch:  49  zinb_loss = 0.68  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.13\n",
      "151673  epoch:  50  zinb_loss = 0.68  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.13\n",
      "151673  epoch:  51  zinb_loss = 0.68  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.12\n",
      "151673  epoch:  52  zinb_loss = 0.68  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.12\n",
      "151673  epoch:  53  zinb_loss = 0.68  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.11\n",
      "151673  epoch:  54  zinb_loss = 0.68  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.11\n",
      "151673  epoch:  55  zinb_loss = 0.68  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.11\n",
      "151673  epoch:  56  zinb_loss = 0.68  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.10\n",
      "151673  epoch:  57  zinb_loss = 0.68  reg_loss = 0.37  con_loss = 0.04  total_loss = 1.10\n",
      "151673  epoch:  58  zinb_loss = 0.67  reg_loss = 0.37  con_loss = 0.04  total_loss = 1.10\n",
      "151673  epoch:  59  zinb_loss = 0.67  reg_loss = 0.37  con_loss = 0.04  total_loss = 1.09\n",
      "151673  epoch:  60  zinb_loss = 0.67  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.09\n",
      "151673  epoch:  61  zinb_loss = 0.67  reg_loss = 0.37  con_loss = 0.04  total_loss = 1.09\n",
      "151673  epoch:  62  zinb_loss = 0.67  reg_loss = 0.37  con_loss = 0.04  total_loss = 1.09\n",
      "151673  epoch:  63  zinb_loss = 0.67  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.08\n",
      "151673  epoch:  64  zinb_loss = 0.67  reg_loss = 0.37  con_loss = 0.04  total_loss = 1.08\n",
      "151673  epoch:  65  zinb_loss = 0.67  reg_loss = 0.38  con_loss = 0.04  total_loss = 1.08\n",
      "151673  epoch:  66  zinb_loss = 0.67  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.07\n",
      "151673  epoch:  67  zinb_loss = 0.67  reg_loss = 0.38  con_loss = 0.04  total_loss = 1.07\n",
      "151673  epoch:  68  zinb_loss = 0.67  reg_loss = 0.38  con_loss = 0.04  total_loss = 1.07\n",
      "151673  epoch:  69  zinb_loss = 0.67  reg_loss = 0.37  con_loss = 0.04  total_loss = 1.07\n",
      "151673  epoch:  70  zinb_loss = 0.67  reg_loss = 0.37  con_loss = 0.04  total_loss = 1.06\n",
      "151673  epoch:  71  zinb_loss = 0.67  reg_loss = 0.37  con_loss = 0.04  total_loss = 1.06\n",
      "151673  epoch:  72  zinb_loss = 0.67  reg_loss = 0.37  con_loss = 0.04  total_loss = 1.06\n",
      "151673  epoch:  73  zinb_loss = 0.67  reg_loss = 0.37  con_loss = 0.04  total_loss = 1.06\n",
      "151673  epoch:  74  zinb_loss = 0.67  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.06\n",
      "151673  epoch:  75  zinb_loss = 0.67  reg_loss = 0.37  con_loss = 0.03  total_loss = 1.05\n",
      "151673  epoch:  76  zinb_loss = 0.67  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.05\n",
      "151673  epoch:  77  zinb_loss = 0.67  reg_loss = 0.37  con_loss = 0.03  total_loss = 1.05\n",
      "151673  epoch:  78  zinb_loss = 0.67  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.05\n",
      "151673  epoch:  79  zinb_loss = 0.66  reg_loss = 0.37  con_loss = 0.03  total_loss = 1.04\n",
      "151673  epoch:  80  zinb_loss = 0.66  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.04\n",
      "151673  epoch:  81  zinb_loss = 0.66  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.04\n",
      "151673  epoch:  82  zinb_loss = 0.66  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.04\n",
      "151673  epoch:  83  zinb_loss = 0.66  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.04\n",
      "151673  epoch:  84  zinb_loss = 0.66  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.04\n",
      "151673  epoch:  85  zinb_loss = 0.66  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.03\n",
      "151673  epoch:  86  zinb_loss = 0.66  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.03\n",
      "151673  epoch:  87  zinb_loss = 0.66  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.03\n",
      "151673  epoch:  88  zinb_loss = 0.66  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.03\n",
      "151673  epoch:  89  zinb_loss = 0.66  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.03\n",
      "151673  epoch:  90  zinb_loss = 0.66  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.03\n",
      "151673  epoch:  91  zinb_loss = 0.66  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.03\n",
      "151673  epoch:  92  zinb_loss = 0.66  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.03\n",
      "151673  epoch:  93  zinb_loss = 0.66  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.03\n",
      "151673  epoch:  94  zinb_loss = 0.66  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.03\n",
      "151673  epoch:  95  zinb_loss = 0.66  reg_loss = 0.37  con_loss = 0.03  total_loss = 1.02\n",
      "151673  epoch:  96  zinb_loss = 0.66  reg_loss = 0.37  con_loss = 0.03  total_loss = 1.02\n",
      "151673  epoch:  97  zinb_loss = 0.66  reg_loss = 0.37  con_loss = 0.03  total_loss = 1.02\n",
      "151673  epoch:  98  zinb_loss = 0.66  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.02\n",
      "151673  epoch:  99  zinb_loss = 0.66  reg_loss = 0.37  con_loss = 0.03  total_loss = 1.01\n",
      "151673  epoch:  100  zinb_loss = 0.66  reg_loss = 0.38  con_loss = 0.03  total_loss = 1.01\n",
      "151673  epoch:  101  zinb_loss = 0.66  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.01\n",
      "151673  epoch:  102  zinb_loss = 0.66  reg_loss = 0.39  con_loss = 0.03  total_loss = 1.02\n",
      "151673  epoch:  103  zinb_loss = 0.66  reg_loss = 0.37  con_loss = 0.03  total_loss = 1.02\n",
      "151673  epoch:  104  zinb_loss = 0.66  reg_loss = 0.38  con_loss = 0.03  total_loss = 1.02\n",
      "151673  epoch:  105  zinb_loss = 0.66  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.01\n",
      "151673  epoch:  106  zinb_loss = 0.66  reg_loss = 0.38  con_loss = 0.03  total_loss = 1.01\n",
      "151673  epoch:  107  zinb_loss = 0.66  reg_loss = 0.37  con_loss = 0.03  total_loss = 1.00\n",
      "151673  epoch:  108  zinb_loss = 0.66  reg_loss = 0.37  con_loss = 0.03  total_loss = 1.01\n",
      "151673  epoch:  109  zinb_loss = 0.66  reg_loss = 0.37  con_loss = 0.03  total_loss = 1.00\n",
      "151673  epoch:  110  zinb_loss = 0.66  reg_loss = 0.37  con_loss = 0.03  total_loss = 1.01\n",
      "151673  epoch:  111  zinb_loss = 0.66  reg_loss = 0.37  con_loss = 0.03  total_loss = 1.01\n",
      "151673  epoch:  112  zinb_loss = 0.66  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.02\n",
      "151673  epoch:  113  zinb_loss = 0.66  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.01\n",
      "151673  epoch:  114  zinb_loss = 0.66  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.05\n",
      "151673  epoch:  115  zinb_loss = 0.66  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.05\n",
      "151673  epoch:  116  zinb_loss = 0.66  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.05\n",
      "151673  epoch:  117  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.05\n",
      "151673  epoch:  118  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.04\n",
      "151673  epoch:  119  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.07  total_loss = 1.36\n",
      "151673  epoch:  120  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.04\n",
      "151673  epoch:  121  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.10  total_loss = 1.69\n",
      "151673  epoch:  122  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.13\n",
      "151673  epoch:  123  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.10  total_loss = 1.66\n",
      "151673  epoch:  124  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.07  total_loss = 1.35\n",
      "151673  epoch:  125  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.14\n",
      "151673  epoch:  126  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.05  total_loss = 1.23\n",
      "151673  epoch:  127  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.05  total_loss = 1.15\n",
      "151673  epoch:  128  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.13\n",
      "151673  epoch:  129  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.05  total_loss = 1.17\n",
      "151673  epoch:  130  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.13\n",
      "151673  epoch:  131  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.12\n",
      "151673  epoch:  132  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.12\n",
      "151673  epoch:  133  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.10\n",
      "151673  epoch:  134  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.09\n",
      "151673  epoch:  135  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.09\n",
      "151673  epoch:  136  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.08\n",
      "151673  epoch:  137  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.07\n",
      "151673  epoch:  138  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.07\n",
      "151673  epoch:  139  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.06\n",
      "151673  epoch:  140  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.06\n",
      "151673  epoch:  141  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.05\n",
      "151673  epoch:  142  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.05\n",
      "151673  epoch:  143  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.04\n",
      "151673  epoch:  144  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.04  total_loss = 1.04\n",
      "151673  epoch:  145  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.03\n",
      "151673  epoch:  146  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.03\n",
      "151673  epoch:  147  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.03\n",
      "151673  epoch:  148  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.02\n",
      "151673  epoch:  149  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.02\n",
      "151673  epoch:  150  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.02\n",
      "151673  epoch:  151  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.01\n",
      "151673  epoch:  152  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.01\n",
      "151673  epoch:  153  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.01\n",
      "151673  epoch:  154  zinb_loss = 0.65  reg_loss = 0.39  con_loss = 0.03  total_loss = 1.01\n",
      "151673  epoch:  155  zinb_loss = 0.65  reg_loss = 0.42  con_loss = 0.03  total_loss = 1.01\n",
      "151673  epoch:  156  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 1.00\n",
      "151673  epoch:  157  zinb_loss = 0.65  reg_loss = 0.42  con_loss = 0.03  total_loss = 1.01\n",
      "151673  epoch:  158  zinb_loss = 0.65  reg_loss = 0.38  con_loss = 0.03  total_loss = 1.00\n",
      "151673  epoch:  159  zinb_loss = 0.65  reg_loss = 0.40  con_loss = 0.03  total_loss = 1.00\n",
      "151673  epoch:  160  zinb_loss = 0.65  reg_loss = 0.37  con_loss = 0.03  total_loss = 0.99\n",
      "151673  epoch:  161  zinb_loss = 0.65  reg_loss = 0.39  con_loss = 0.03  total_loss = 0.99\n",
      "151673  epoch:  162  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.99\n",
      "151673  epoch:  163  zinb_loss = 0.65  reg_loss = 0.38  con_loss = 0.03  total_loss = 0.99\n",
      "151673  epoch:  164  zinb_loss = 0.65  reg_loss = 0.37  con_loss = 0.03  total_loss = 0.99\n",
      "151673  epoch:  165  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.98\n",
      "151673  epoch:  166  zinb_loss = 0.65  reg_loss = 0.37  con_loss = 0.03  total_loss = 0.98\n",
      "151673  epoch:  167  zinb_loss = 0.65  reg_loss = 0.37  con_loss = 0.03  total_loss = 0.98\n",
      "151673  epoch:  168  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.98\n",
      "151673  epoch:  169  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.98\n",
      "151673  epoch:  170  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.98\n",
      "151673  epoch:  171  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.98\n",
      "151673  epoch:  172  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.97\n",
      "151673  epoch:  173  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.97\n",
      "151673  epoch:  174  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.97\n",
      "151673  epoch:  175  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.97\n",
      "151673  epoch:  176  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.97\n",
      "151673  epoch:  177  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.97\n",
      "151673  epoch:  178  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.97\n",
      "151673  epoch:  179  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.97\n",
      "151673  epoch:  180  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.97\n",
      "151673  epoch:  181  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.97\n",
      "151673  epoch:  182  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.96\n",
      "151673  epoch:  183  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.96\n",
      "151673  epoch:  184  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.96\n",
      "151673  epoch:  185  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.96\n",
      "151673  epoch:  186  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.96\n",
      "151673  epoch:  187  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.96\n",
      "151673  epoch:  188  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.96\n",
      "151673  epoch:  189  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.96\n",
      "151673  epoch:  190  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.96\n",
      "151673  epoch:  191  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.96\n",
      "151673  epoch:  192  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.96\n",
      "151673  epoch:  193  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.96\n",
      "151673  epoch:  194  zinb_loss = 0.65  reg_loss = 0.36  con_loss = 0.03  total_loss = 0.95\n",
      "151673  epoch:  195  zinb_loss = 0.65  reg_loss = 0.35  con_loss = 0.03  total_loss = 0.95\n",
      "151673  epoch:  196  zinb_loss = 0.65  reg_loss = 0.35  con_loss = 0.03  total_loss = 0.95\n",
      "151673  epoch:  197  zinb_loss = 0.65  reg_loss = 0.35  con_loss = 0.03  total_loss = 0.95\n",
      "151673  epoch:  198  zinb_loss = 0.65  reg_loss = 0.35  con_loss = 0.03  total_loss = 0.95\n",
      "151673  epoch:  199  zinb_loss = 0.65  reg_loss = 0.35  con_loss = 0.03  total_loss = 0.95\n",
      "151673  epoch:  200  zinb_loss = 0.65  reg_loss = 0.35  con_loss = 0.03  total_loss = 0.95\n",
      "Current memory usage_end:ï¼š2.2718 GB\n",
      "time: 157.4116 s\n",
      "memory blocks peak:    0.1353 GB\n",
      "              ari       nmi       ami        sc   ari_max   nmi_max   ami_max  \\\n",
      "dataset                                                                         \n",
      "151673   0.394362  0.581096  0.579824  0.463116  0.499115  0.658409  0.657357   \n",
      "\n",
      "           SC_max        time      Memo  Memo_peak  round  \n",
      "dataset                                                    \n",
      "151673   0.418743  157.411613  2.271778   0.135321      1  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    " parse = argparse.ArgumentParser()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "Dataset_test= [ '151673']\n",
    "for dataset in Dataset_test:\n",
    "    if dataset.startswith('15'):\n",
    "        config_file = './config/DLPFC.ini'\n",
    "        savepath = f'../../Output/Spatial_MGCN/DLPFC/{dataset}/'\n",
    "        adata, features, labels, sadj, fadj, graph_nei, graph_neg = load_DLPFC_data(dataset)\n",
    "    else:\n",
    "        config_file = './config/' + dataset + '.ini'\n",
    "        savepath = f'../../Output/Spatial_MGCN/{dataset}/'\n",
    "        adata, features, labels, sadj, fadj, graph_nei, graph_neg = load_data(dataset)\n",
    "    mk_dir(savepath)\n",
    "\n",
    "\n",
    "    config = Config(config_file)\n",
    "    cuda = not config.no_cuda and torch.cuda.is_available()\n",
    "    use_seed = not config.no_seed\n",
    "\n",
    "    _, ground = np.unique(np.array(labels, dtype=str), return_inverse=True)\n",
    "    ground = torch.LongTensor(ground)\n",
    "    config.n = len(ground)\n",
    "    config.class_num = len(ground.unique())\n",
    "\n",
    "    results = pd.DataFrame()\n",
    "    for i in range(1):\n",
    "        num = i + 1\n",
    "        print(\"===Train epoch{}====\".format(num))\n",
    "        start = time.time()\n",
    "        tracemalloc.start()\n",
    "        start_MB = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024 / 1024\n",
    "\n",
    "        if cuda:\n",
    "            features = features.cuda()\n",
    "            sadj = sadj.cuda()\n",
    "            fadj = fadj.cuda()\n",
    "            graph_nei = graph_nei.cuda()\n",
    "            graph_neg = graph_neg.cuda()\n",
    "\n",
    "        config.epochs = config.epochs + 1\n",
    "\n",
    "        np.random.seed(config.seed)\n",
    "        torch.cuda.manual_seed(config.seed)\n",
    "        random.seed(config.seed)\n",
    "        np.random.seed(config.seed)\n",
    "        torch.manual_seed(config.seed)\n",
    "        os.environ['PYTHONHASHSEED'] = str(config.seed)\n",
    "        if not config.no_cuda and torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(config.seed)\n",
    "            torch.cuda.manual_seed_all(config.seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "\n",
    "        print(dataset, ' ', config.lr, ' ', config.alpha, ' ', config.beta, ' ', config.gamma)\n",
    "        model = Spatial_MGCN(nfeat=config.fdim,\n",
    "                             nhid1=config.nhid1,\n",
    "                             nhid2=config.nhid2,\n",
    "                             dropout=config.dropout)\n",
    "        if cuda:\n",
    "            model.cuda()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config.lr,\n",
    "                               weight_decay=config.weight_decay)\n",
    "        epoch_max = 0\n",
    "        ari_max = 0\n",
    "        idx_max = []\n",
    "        mean_max = []\n",
    "        emb_max = []\n",
    "        for epoch in range(config.epochs):\n",
    "            emb, mean, zinb_loss, reg_loss, con_loss, total_loss = train()\n",
    "            print(dataset, ' epoch: ', epoch, ' zinb_loss = {:.2f}'.format(zinb_loss),\n",
    "                  ' reg_loss = {:.2f}'.format(reg_loss), ' con_loss = {:.2f}'.format(con_loss),\n",
    "                  ' total_loss = {:.2f}'.format(total_loss))\n",
    "            kmeans = KMeans(n_clusters=config.class_num).fit(emb)\n",
    "            idx = kmeans.labels_\n",
    "\n",
    "            if dataset.startswith('15'):\n",
    "                df = pd.DataFrame({'labels': labels, 'idx': idx})\n",
    "                df_cleaned = df.dropna()\n",
    "                ground_filter = df_cleaned['labels'].values\n",
    "                pred_filter = df_cleaned['idx'].values\n",
    "                ari_res = metrics.adjusted_rand_score(ground_filter, pred_filter)\n",
    "            else:\n",
    "                ari_res = metrics.adjusted_rand_score(labels, idx)\n",
    "\n",
    "            if ari_res > ari_max:\n",
    "                ari_max = ari_res\n",
    "                epoch_max = epoch\n",
    "                idx_max = idx\n",
    "                mean_max = mean\n",
    "                emb_max = emb\n",
    "\n",
    "        end = time.time()\n",
    "        end_MB = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024 / 1024\n",
    "        uesd_time = end - start\n",
    "        used_memo = end_MB - start_MB\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "        peak = peak / 1024.0 / 1024.0 / 1024.0\n",
    "        print(u'Current memory usage_end:ï¼š%.4f GB' % used_memo)\n",
    "        print('time: {:.4f} s'.format(uesd_time))\n",
    "        print('memory blocks peak:{:>10.4f} GB'.format(peak))\n",
    "        tracemalloc.clear_traces()\n",
    "        label_df = pd.DataFrame({\"True\": labels, \"Pred\": idx,\"pred_max\":idx_max}).dropna()\n",
    "\n",
    "\n",
    "        ari,nmi,ami= eval_model(label_df[\"Pred\"],label_df[\"True\"])\n",
    "        SC = silhouette_score(emb, idx)\n",
    "        res = {}\n",
    "        res[\"dataset\"] = dataset\n",
    "        res[\"ari\"] = ari\n",
    "        res[\"nmi\"] = nmi\n",
    "        res[\"ami\"] = ami\n",
    "        res[\"sc\"] = SC\n",
    "\n",
    "        ari_max = adjusted_rand_score(label_df[\"pred_max\"],label_df[\"True\"])\n",
    "        nmi_max = normalized_mutual_info_score(label_df[\"pred_max\"],label_df[\"True\"])\n",
    "        ami_max = adjusted_mutual_info_score(label_df[\"pred_max\"],label_df[\"True\"])\n",
    "        res['ari_max'] = ari_max\n",
    "        res['nmi_max'] = nmi_max\n",
    "        res['ami_max'] = ami_max\n",
    "        res['SC_max'] = silhouette_score(emb_max, idx_max)\n",
    "        res[\"time\"] = uesd_time\n",
    "        res[\"Memo\"] = used_memo\n",
    "        res[\"Memo_peak\"] = peak\n",
    "        res[\"round\"] = i + 1\n",
    "        results = results._append(res, ignore_index=True)\n",
    "\n",
    "    results.set_index('dataset', inplace=True)\n",
    "    print(results.head())\n",
    "    results.to_csv(os.path.join(savepath, \"result_scores.csv\"))\n",
    "\n",
    "    res_mean = results.mean()\n",
    "    res_mean.to_csv(f'{savepath}{dataset}_mean.csv', header=True)\n",
    "    res_std = results.std()\n",
    "    res_std.to_csv(f'{savepath}{dataset}_std.csv', header=True)\n",
    "    res_median = results.median()\n",
    "    res_median.to_csv(f'{savepath}{dataset}_median.csv', header=True)\n",
    "\n",
    "    adata.obs['idx'] = idx_max.astype(str)\n",
    "    adata.obsm['emb'] = emb_max\n",
    "    adata.obsm['mean'] = mean_max\n",
    "    if config.gamma == 0:\n",
    "        title = 'Spatial_MGCN-w/o'\n",
    "        pd.DataFrame(emb_max).to_csv(savepath + 'Spatial_MGCN_no_emb.csv', header=None, index=None)\n",
    "        pd.DataFrame(idx_max).to_csv(savepath + 'Spatial_MGCN_no_idx.csv', header=None, index=None)\n",
    "        sc.pl.spatial(adata, img_key=\"hires\", color=['idx'], title=title, show=False)\n",
    "        plt.savefig(savepath + 'Spatial_MGCN_no.jpg', bbox_inches='tight', dpi=600)\n",
    "        plt.show()\n",
    "    else:\n",
    "        title = 'Spatial_MGCN'\n",
    "        pd.DataFrame(emb_max).to_csv(savepath + 'Spatial_MGCN_emb.csv', header=None, index=None)\n",
    "        pd.DataFrame(idx_max).to_csv(savepath + 'Spatial_MGCN_idx.csv', header=None, index=None)\n",
    "        adata.layers['X'] = adata.X\n",
    "        adata.layers['mean'] = mean_max\n",
    "        # adata.write(f'{savepath}Spatial_MGCN_{dataset}.h5ad' )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
