{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, adjusted_mutual_info_score,silhouette_samples\n",
    "from sklearn.metrics import silhouette_score\n",
    "from src.utils_func import plot_clustering\n",
    "import psutil,time,tracemalloc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "def eval_model(pred, labels=None):\n",
    "    if labels is not None:\n",
    "        label_df = pd.DataFrame({\"True\": labels, \"Pred\": pred}).dropna()\n",
    "        ari = adjusted_rand_score(label_df[\"True\"], label_df[\"Pred\"])\n",
    "        nmi = normalized_mutual_info_score(label_df[\"True\"], label_df[\"Pred\"])\n",
    "        ami=adjusted_mutual_info_score(label_df[\"True\"], label_df[\"Pred\"])\n",
    "    return  ari,nmi,ami\n",
    "\n",
    "def mk_dir(input_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        os.makedirs(input_path)\n",
    "    return input_path\n",
    "\n",
    "# set seed before every run\n",
    "def seed_torch(seed):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import os\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def refine_function(sample_id, pred, dis, shape=\"hexagon\"):\n",
    "    refined_pred = []\n",
    "    pred = pd.DataFrame({\"pred\": pred}, index=sample_id)\n",
    "    dis_df = pd.DataFrame(dis, index=sample_id, columns=sample_id)\n",
    "    if shape == \"hexagon\":\n",
    "        num_nbs = 6\n",
    "    elif shape == \"square\":\n",
    "        num_nbs = 4\n",
    "    else:\n",
    "        print(\n",
    "            \"Shape not recongized, shape='hexagon' for Visium data, 'square' for ST data.\")\n",
    "    for i in range(len(sample_id)):\n",
    "        index = sample_id[i]\n",
    "        dis_tmp = dis_df.loc[index, :].sort_values(ascending=False)\n",
    "        nbs = dis_tmp[0:num_nbs+1]\n",
    "        nbs_pred = pred.loc[nbs.index, \"pred\"]\n",
    "        self_pred = pred.loc[index, \"pred\"]\n",
    "        v_c = nbs_pred.value_counts()\n",
    "        if (v_c.loc[self_pred] < num_nbs/2) and (np.max(v_c) > num_nbs/2):\n",
    "            refined_pred.append(v_c.idxmax())\n",
    "        else:\n",
    "            refined_pred.append(self_pred)\n",
    "    return refined_pred\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def run_conST(adata_h5, dataset,\n",
    "              device=torch.device(\n",
    "                  'cuda:0' if torch.cuda.is_available() else 'cpu'),\n",
    "              save_data_path=\"../../Output/conST/\",\n",
    "              n_clusters=6):\n",
    "    import sys\n",
    "    # sys.path.append(\"/home/fangzy/project/st_cluster/code/methods/conST-main\")\n",
    "    from src.graph_func import graph_construction\n",
    "    from src.utils_func import mk_dir, adata_preprocess, load_ST_file, res_search_fixed_clus, plot_clustering\n",
    "    from src.training import conST_training\n",
    "    import argparse\n",
    "    import numpy as np\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--k', type=int, default=10,\n",
    "                        help='parameter k in spatial graph')\n",
    "    parser.add_argument('--knn_distanceType', type=str, default='euclidean',\n",
    "                        help='graph distance type: euclidean/cosine/correlation')\n",
    "    parser.add_argument('--epochs', type=int, default=200,\n",
    "                        help='Number of epochs to train.') #200\n",
    "    parser.add_argument('--cell_feat_dim', type=int,\n",
    "                        default=300, help='Dim of PCA')\n",
    "    parser.add_argument('--feat_hidden1', type=int,\n",
    "                        default=100, help='Dim of DNN hidden 1-layer.')\n",
    "    parser.add_argument('--feat_hidden2', type=int,\n",
    "                        default=20, help='Dim of DNN hidden 2-layer.')\n",
    "    parser.add_argument('--gcn_hidden1', type=int, default=32,\n",
    "                        help='Dim of GCN hidden 1-layer.')\n",
    "    parser.add_argument('--gcn_hidden2', type=int, default=8,\n",
    "                        help='Dim of GCN hidden 2-layer.')\n",
    "    parser.add_argument('--p_drop', type=float,\n",
    "                        default=0.2, help='Dropout rate.')\n",
    "    parser.add_argument('--use_img', type=bool,\n",
    "                        default=False, help='Use histology images.')\n",
    "    parser.add_argument('--img_w', type=float, default=0.1,\n",
    "                        help='Weight of image features.')\n",
    "    parser.add_argument('--use_pretrained', type=bool,\n",
    "                        default=True, help='Use pretrained weights.')\n",
    "    parser.add_argument('--using_mask', type=bool,\n",
    "                        default=False, help='Using mask for multi-dataset.')\n",
    "    parser.add_argument('--feat_w', type=float, default=10,\n",
    "                        help='Weight of DNN loss.')\n",
    "    parser.add_argument('--gcn_w', type=float, default=0.1,\n",
    "                        help='Weight of GCN loss.')\n",
    "    parser.add_argument('--dec_kl_w', type=float,\n",
    "                        default=10, help='Weight of DEC loss.')\n",
    "    parser.add_argument('--gcn_lr', type=float, default=0.01,\n",
    "                        help='Initial GNN learning rate.')\n",
    "    parser.add_argument('--gcn_decay', type=float,\n",
    "                        default=0.01, help='Initial decay rate.')\n",
    "    parser.add_argument('--dec_cluster_n', type=int,\n",
    "                        default=10, help='DEC cluster number.')\n",
    "    parser.add_argument('--dec_interval', type=int,\n",
    "                        default=20, help='DEC interval nnumber.')\n",
    "    parser.add_argument('--dec_tol', type=float, default=0.00, help='DEC tol.')\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=0, help='random seed')\n",
    "    parser.add_argument('--beta', type=float, default=100,\n",
    "                        help='beta value for l2c')\n",
    "    parser.add_argument('--cont_l2l', type=float, default=0.3,\n",
    "                        help='Weight of local contrastive learning loss.')\n",
    "    parser.add_argument('--cont_l2c', type=float, default=0.1,\n",
    "                        help='Weight of context contrastive learning loss.')\n",
    "    parser.add_argument('--cont_l2g', type=float, default=0.1,\n",
    "                        help='Weight of global contrastive learning loss.')\n",
    "\n",
    "    parser.add_argument('--edge_drop_p1', type=float, default=0.1,\n",
    "                        help='drop rate of adjacent matrix of the first view')\n",
    "    parser.add_argument('--edge_drop_p2', type=float, default=0.1,\n",
    "                        help='drop rate of adjacent matrix of the second view')\n",
    "    parser.add_argument('--node_drop_p1', type=float, default=0.2,\n",
    "                        help='drop rate of node features of the first view')\n",
    "    parser.add_argument('--node_drop_p2', type=float, default=0.3,\n",
    "                        help='drop rate of node features of the second view')\n",
    "\n",
    "    # ______________ Eval clustering Setting ______________\n",
    "    parser.add_argument('--eval_resolution', type=int,\n",
    "                        default=1, help='Eval cluster number.')\n",
    "    parser.add_argument('--eval_graph_n', type=int,\n",
    "                        default=20, help='Eval graph kN tol.')\n",
    "\n",
    "    params = parser.parse_args(args=['--k', '10', '--knn_distanceType',\n",
    "                               'euclidean', '--epochs', '200', '--use_pretrained', 'False'])\n",
    "\n",
    "\n",
    "    params.device = device\n",
    "    params.save_path = mk_dir(f'{save_data_path}/{dataset}')\n",
    "    # start = time.time()\n",
    "    # start_MB = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024 / 1024\n",
    "    adata_X = adata_preprocess(adata_h5, min_cells=5, pca_n_comps=params.cell_feat_dim) #300\n",
    "    graph_dict = graph_construction(adata_h5.obsm['spatial'], adata_h5.shape[0], params)\n",
    "    params.cell_num = adata_h5.shape[0]\n",
    "\n",
    "    if (\"ground_truth\" in adata_h5.obs.keys()):\n",
    "        n_clusters = len(set(adata_h5.obs[\"ground_truth\"].dropna()))\n",
    "    else:\n",
    "        n_clusters = n_clusters\n",
    "\n",
    "    print(\"===Dataset:{},n_clusters:{}====\".format(dataset,n_clusters))\n",
    "\n",
    "    if params.use_img:\n",
    "        img_transformed = np.load('./MAE-pytorch/extracted_feature.npy')\n",
    "        img_transformed = (img_transformed - img_transformed.mean()) / \\\n",
    "            img_transformed.std() * adata_X.std() + adata_X.mean()\n",
    "        conST_net = conST_training(\n",
    "            adata_X, graph_dict, params, n_clusters, img_transformed)\n",
    "    else:\n",
    "        conST_net = conST_training(adata_X, graph_dict, params, n_clusters)\n",
    "\n",
    "    conST_net.pretraining()\n",
    "    conST_net.major_training()\n",
    "\n",
    "    conST_embedding = conST_net.get_embedding()\n",
    "\n",
    "    # np.save(f'{params.save_path}/conST_result.npy', conST_embedding)\n",
    "    adata_h5.obsm[\"embedding\"] = conST_embedding\n",
    "    sc.pp.neighbors(adata_h5, n_neighbors=params.eval_graph_n,use_rep='embedding')\n",
    "    eval_resolution = res_search_fixed_clus(adata_h5, n_clusters)\n",
    "    print(\"resolution:\",eval_resolution)\n",
    "    cluster_key = \"conST_leiden\"\n",
    "    sc.tl.leiden(adata_h5, key_added=cluster_key, resolution=eval_resolution)\n",
    "\n",
    "    index = np.arange(start=0, stop=adata_X.shape[0]).tolist()\n",
    "    index = [str(x) for x in index]\n",
    "\n",
    "    dis = graph_dict['adj_norm'].to_dense().numpy(\n",
    "    ) + np.eye(graph_dict['adj_norm'].shape[0])\n",
    "\n",
    "    if dataset.startswith('15'):\n",
    "        dataset='DLPFC'\n",
    "    refine_map = {\"Breast_cancer\": \"hexagon\", \"Mouse_brain\": \"hexagon\",\n",
    "                   \"Mouse_olfactory\": \"hexagon\",\"DLPFC\":\"hexagon\" }\n",
    "    refine = refine_function(sample_id=index, shape=refine_map[dataset],\n",
    "                             pred=adata_h5.obs['leiden'].tolist(), dis=dis)\n",
    "    # end = time.time()\n",
    "    # end_MB = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024 / 1024  #\n",
    "    # used_memory = end_MB - start_MB\n",
    "\n",
    "    res = {}\n",
    "    adata_h5.obs['refine'] = refine\n",
    "    if (\"ground_truth\" in adata_h5.obs.keys()):\n",
    "        ari_r,nmi_r,ami_r = eval_model(adata_h5.obs['refine'], adata_h5.obs['ground_truth'])\n",
    "        ari,nmi,ami= eval_model(adata_h5.obs['conST_leiden'], adata_h5.obs['ground_truth'])\n",
    "        print(\"adata_h5.obsm[embedding].shape\",adata_h5.obsm[\"embedding\"].shape)\n",
    "        print(\"adata_h5.obs['conST_leiden']\",adata_h5.obs['conST_leiden'].value_counts())\n",
    "        SC = silhouette_score(adata_h5.obsm[\"embedding\"], adata_h5.obs['conST_leiden'])\n",
    "        SC_r = silhouette_score(adata_h5.obsm[\"embedding\"], adata_h5.obs['refine'])\n",
    "\n",
    "        used_adata = adata_h5[adata_h5.obs[\"ground_truth\"].notna()] #因为ground_truth中可能Na,所以筛选去除\n",
    "        SC_revise = silhouette_score(used_adata.obsm[\"embedding\"], used_adata.obs['ground_truth'])\n",
    "        print(\"SC_revise:\", SC_revise)\n",
    "\n",
    "\n",
    "        res[\"dataset\"] = dataset\n",
    "        res[\"ari\"] = ari\n",
    "        res[\"nmi\"] = nmi\n",
    "        res[\"ami\"] = ami\n",
    "        res[\"sc\"] = SC\n",
    "        res[\"SC_revise\"] =SC_revise\n",
    "\n",
    "        res[\"ari_1\"] = ari_r\n",
    "        res[\"nmi_1\"] = nmi_r\n",
    "        res[\"ami_1\"] = ami_r\n",
    "        res[\"sc_1\"] = SC_r\n",
    "    adata_h5.obs[\"pred_label\"] = refine\n",
    "\n",
    "    return res, adata_h5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory usage_end:：0.0000 GB\n",
      "time: 0.0000 s\n",
      "memory blocks peak:    0.0000 GB\n",
      "load DLPFC dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda_install\\envs\\STAGATE\\lib\\site-packages\\anndata\\_core\\anndata.py:1830: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3639, 33538)\n",
      "===Training epoch:1====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "J:\\第一篇论文所有代码备份\\深度学习测评综述数据及代码，结果_放入Github中\\Benchmark_SRT-main (demo展示)\\conST\\src\\graph_func.py:75: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj_org = nx.adjacency_matrix(nx.from_dict_of_lists(graphdict)) #(3798,3798)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Dataset:151673,n_clusters:7====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[KPretraining stage:  |################| 200 / 200 | Left time: 0.00 mins| Loss: 133.0694\n",
      "\u001B[?25hD:\\Anaconda_install\\envs\\STAGATE\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'dropout_adj' is deprecated, use 'dropout_edge' instead\n",
      "  warnings.warn(out)\n",
      "J:\\第一篇论文所有代码备份\\深度学习测评综述数据及代码，结果_放入Github中\\Benchmark_SRT-main (demo展示)\\conST\\src\\training.py:239: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss_kl = F.kl_div(out_q.log(), torch.tensor(tmp_p).to(self.device)).to(self.device)\n",
      "D:\\Anaconda_install\\envs\\STAGATE\\lib\\site-packages\\torch\\nn\\functional.py:2916: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "\u001B[KMajor training stage:  |################| 200 / 200 | Loss: 3.3628\n",
      "\u001B[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resolution: 0.38999999999999996\n",
      "adata_h5.obsm[embedding].shape (3639, 28)\n",
      "adata_h5.obs['conST_leiden'] 0    1047\n",
      "1     797\n",
      "2     498\n",
      "3     445\n",
      "4     322\n",
      "5     267\n",
      "6     263\n",
      "Name: conST_leiden, dtype: int64\n",
      "SC_revise: 0.13364899\n",
      "Current memory usage_end:：2.7576 GB\n",
      "time: 110.0492 s\n",
      "memory blocks peak:    0.9340 GB\n",
      "              ari       nmi       ami        sc  SC_revise     ari_1  \\\n",
      "dataset                                                                \n",
      "DLPFC    0.444706  0.651101  0.650134  0.396435   0.133649  0.447296   \n",
      "\n",
      "            nmi_1     ami_1      sc_1       time      Memo  Memo_peak  round  \n",
      "dataset                                                                       \n",
      "DLPFC    0.661431  0.660492  0.390686  110.04922  2.757626    0.93402      1  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils_for_all as usa\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    dataset2 = ['151507', '151508', '151509', '151510','151669', '151670', '151671', '151672', '151673', '151674', '151675', '151676']\n",
    "    Dataset_test = ['151673']\n",
    "\n",
    "for dataset in Dataset_test:\n",
    "    best_ari = 0\n",
    "    if dataset.startswith('15'):\n",
    "        save_path =f'../../Output/conST/DLPFC/{dataset}/'\n",
    "    else:\n",
    "        save_path = f'../../Output/conST/{dataset}/'\n",
    "    mk_dir(save_path)\n",
    "\n",
    "    adata, n_clusters = usa.get_adata(dataset, data_path='../../Dataset/')\n",
    "    adata.var_names_make_unique()\n",
    "    print(adata.shape)\n",
    "\n",
    "\n",
    "    results = pd.DataFrame()\n",
    "    for i in range(1):\n",
    "        num=i+1\n",
    "        print(\"===Training epoch:{}====\".format(num))\n",
    "        start = time.time()\n",
    "        tracemalloc.start()\n",
    "        start_MB = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024 / 1024\n",
    "        res, adata_h5 = run_conST(adata.copy(), dataset,n_clusters=n_clusters)\n",
    "\n",
    "        end = time.time()\n",
    "        end_MB = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024 / 1024\n",
    "        uesd_time = end - start\n",
    "        used_memo = end_MB - start_MB\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "        peak = peak / 1024.0 / 1024.0 / 1024.0\n",
    "        print(u'Current memory usage_end:：%.4f GB' % used_memo)\n",
    "        print('time: {:.4f} s'.format(uesd_time))\n",
    "        print('memory blocks peak:{:>10.4f} GB'.format(peak))\n",
    "        tracemalloc.clear_traces()\n",
    "        res[\"time\"] = end - start\n",
    "        res[\"Memo\"] =  used_memo\n",
    "        res[\"Memo_peak\"] = peak\n",
    "        res[\"round\"] = i+1\n",
    "        results = results._append(res, ignore_index=True)\n",
    "        results.to_csv(save_path + \"result.csv\", header=True)\n",
    "        adata_h5.write_h5ad(save_path+str(dataset)+\".h5ad\")\n",
    "\n",
    "        if dataset in [\"Breast_cancer\", \"Mouse_brain\",\"Stereo\"]:\n",
    "            key='leiden'\n",
    "            savepath = f'{save_path}/conST_leiden_plot.jpg'\n",
    "            plot_clustering(adata_h5, key, savepath=savepath)\n",
    "            cluster_key = 'refine'\n",
    "            savepath = f'{save_path}/conST_leiden_plot_refined.jpg'\n",
    "            plot_clustering(adata_h5, cluster_key, savepath=savepath)\n",
    "\n",
    "    results.set_index('dataset', inplace=True)\n",
    "    print(results.head())\n",
    "    res_mean = results.mean()\n",
    "    res_mean.to_csv(f'{save_path}{dataset}_mean.csv', header=True)\n",
    "    res_std = results.std()\n",
    "    res_std.to_csv(f'{save_path}{dataset}_std.csv', header=True)\n",
    "    res_median = results.median()\n",
    "    res_median.to_csv(f'{save_path}{dataset}_median.csv', header=True) #\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
